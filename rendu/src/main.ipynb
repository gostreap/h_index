{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Challenge: H-index Prediction\n",
    "\n",
    "Michael Fotso Fotso, Tristan Fran√ßois and Christian Kotait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First preprocessing\n",
    "\n",
    "Create a file `processed_data.csv` in the folde `../tmp/` with a few features related to the graph and two features related to a **fasttext model**.\n",
    "\n",
    "It took about 15 minutes on our machines.\n",
    "\n",
    "You can now run the code in the last section to test the performance locally. You should get a MSE of about 52 with 1000 iterations in just a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_utils import store_full_dataset_with_features\n",
    "from utils import write_train_data_json\n",
    "\n",
    "write_train_data_json()\n",
    "store_full_dataset_with_features(from_scratch=True, vectorize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add authority feature\n",
    "\n",
    "It took about 10 minutes on our machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_utils import PROCESSED_DATA_PATH, add_authority, get_processed_data\n",
    "\n",
    "data = get_processed_data(split=False)\n",
    "data = add_authority(data)\n",
    "data.to_csv(PROCESSED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a closeness centrality feature\n",
    "\n",
    "It took about 2 hours on our machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_closeness\n",
    "from preprocess_utils import PROCESSED_DATA_PATH, add_features, get_processed_data\n",
    "\n",
    "data = get_processed_data(split=False)\n",
    "closeness = get_closeness()\n",
    "data = add_features(data, closeness)\n",
    "data.to_csv(PROCESSED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add doc2vec features\n",
    "\n",
    "Add 20 features from a doc2vec model.\n",
    "\n",
    "It took about 5 hours on our machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2vec import add_do2vec_to_whole_dataset\n",
    "from preprocess_utils import PROCESSED_DATA_PATH, get_processed_data\n",
    "\n",
    "data = get_processed_data(split=False)\n",
    "data = add_do2vec_to_whole_dataset(data)\n",
    "data.to_csv(PROCESSED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add tf-idf features\n",
    "\n",
    "Add `n_features` from a tf-idf vectorizer.\n",
    "\n",
    "It took a few minutes on our machine but it requires a lot of ram. \n",
    "\n",
    "We were able to generate up to 5000 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_utils import PROCESSED_DATA_PATH, get_processed_data, add_tf_idf\n",
    "\n",
    "data = get_processed_data(split=False)\n",
    "data = add_tf_idf(data, n_features=1000)\n",
    "data.to_csv(PROCESSED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and submission\n",
    "\n",
    "The variables `iterations` and `task_type` define respectively the number of iterations in the model training and the execution mode. If you do not have a GPU, set task_type to CPU.\n",
    "\n",
    "The execution time of the model depends strongly on the number of tf-idf features selected.\n",
    "\n",
    "We were able to train with 150,000 iterations with 3000 tf-idf features in a few hours on our machines.\n",
    "\n",
    "However, we were far from converging and increasing the number of iterations and/or features would most likely have improved the results significantly. However, we had neither the time nor the necessary equipment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 1000\n",
    "task_type = \"GPU\" # \"GPU\" or \"CPU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from preprocess_utils import get_submission_data\n",
    "from read_data import get_test_data\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_submission_data()\n",
    "\n",
    "model_cat = CatBoostRegressor(\n",
    "    verbose=False,\n",
    "    random_state=1,\n",
    "    iterations=iterations,\n",
    "    task_type=task_type,\n",
    "    depth=8\n",
    ")\n",
    "\n",
    "model_cat.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_cat.predict(X_test)\n",
    "\n",
    "test, _ = get_test_data()\n",
    "test[\"hindex\"] = y_pred\n",
    "submission = test[[\"author\", \"hindex\"]]\n",
    "submission.to_csv(\"../tmp/submission.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run for local test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from preprocess_utils import get_split_train_data\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_split_train_data()\n",
    "\n",
    "model_cat = CatBoostRegressor(\n",
    "    verbose=False,\n",
    "    iterations=iterations,\n",
    "    task_type=task_type,\n",
    "    depth=8\n",
    ")\n",
    "\n",
    "model_cat.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_cat.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d6c35add06951c3fa2bdd031e08b3c25f53dc276e46f866b05e35e29dd5c2bf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
