{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse et comprehension des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from read_data import *\n",
    "from utils import *\n",
    "from fasttext_utils import *\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "217802\n",
      "Starting data columns : ['author', 'hindex', 'text', 'nb_paper']\n",
      "Add core number to data\n",
      "Add pagerank to data\n",
      "Add authority to data\n",
      "Add clustering coef to data\n",
      "Add eigenvector centrality to data\n",
      "Add small class to data\n",
      "Add neighborhood info to data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217801/217801 [00:55<00:00, 3935.78it/s]\n",
      "Read 99M words\n",
      "Number of words:  359997\n",
      "Number of labels: 6\n",
      "Progress: 100.0% words/sec/thread: 6841519 lr:  0.000000 avg.loss:  1.123567 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending data columns : ['author', 'hindex', 'text', 'nb_paper', 'core_number', 'pagerank', 'authority', 'clustering_coef', 'eigenvector_centrality', 'modindx', 'hindex_lab', 'n_neighbors_dist_1', 'min_neighbors_dist_1', 'mean_neighbors_dist_1', 'max_neighbors_dist_1', 'n_neighbors_dist_2', 'min_neighbors_dist_2', 'mean_neighbors_dist_2', 'max_neighbors_dist_2', 'vector_coord_0', 'vector_coord_1']\n"
     ]
    }
   ],
   "source": [
    "store_full_dataset_with_features(from_scratch=True, vectorize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/57/vt366_td3_37y7bqh4rk7nqr0000gn/T/ipykernel_11641/1429269039.py:1: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  train, test = get_processed_data()\n"
     ]
    }
   ],
   "source": [
    "train, test = get_processed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "G, _, _ = get_graph()\n",
    "# G = nx.convert_matrix.to_scipy_sparse_matrix(G)\n",
    "percolation_centralities = nx.algorithms.centrality.eigenvector_centrality(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**distribution des données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,n_train = get_train_data()\n",
    "test,n_test = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(x=train['hindex'],bins=187);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**exploitation de model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On est sur que le modèle applique softmax sur la représentation du mot. Nous ou choisit donc d'appliquer un autre régresseur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data for multiple regression model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/57/vt366_td3_37y7bqh4rk7nqr0000gn/T/ipykernel_11641/2692092469.py:1: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  X_train, y_train, X_test, y_test = get_numpy_data(n=174241)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = get_numpy_data(n=174241)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm, X_test_norm = normalize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XG_BOOST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xg = XGBRegressor(learning_rate=0.01, max_depth=5, objective=\"reg:linear\", subsample=0.7, n_estimators=500)\n",
    "model_xg.fit(X_train,y_train)\n",
    "mod_preds = model_xg.predict(X_test)\n",
    "mean_squared_error(y_test,mod_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn = keras.Sequential()\n",
    "model_nn.add(layers.Dense(64, kernel_initializer='uniform', input_shape=(10,)))\n",
    "model_nn.add(layers.Activation('softmax'))\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-2,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "\n",
    "model_nn.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "model_nn.fit(X_train,y_train, epochs=10, batch_size=5)\n",
    "mod_nn_preds = model_nn.predict(X_test)\n",
    "print(len(mod_nn_preds))\n",
    "mean_squared_error(y_test,mod_nn_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CATBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.3607035691917"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "model_cat = CatBoostRegressor(verbose=False)\n",
    "model_cat.fit(X_train, y_train)\n",
    "mod_preds = model_cat.predict(X_test)\n",
    "mean_squared_error(y_test,mod_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold [0/3]\n",
      "\n",
      "bestTest = 6.943675114\n",
      "bestIteration = 997\n",
      "\n",
      "Training on fold [1/3]\n",
      "\n",
      "bestTest = 7.025288715\n",
      "bestIteration = 998\n",
      "\n",
      "Training on fold [2/3]\n",
      "\n",
      "bestTest = 7.019548334\n",
      "bestIteration = 999\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iterations</th>\n",
       "      <th>test-RMSE-mean</th>\n",
       "      <th>test-RMSE-std</th>\n",
       "      <th>train-RMSE-mean</th>\n",
       "      <th>train-RMSE-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>15.769091</td>\n",
       "      <td>0.144416</td>\n",
       "      <td>15.768537</td>\n",
       "      <td>0.075459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15.453130</td>\n",
       "      <td>0.140421</td>\n",
       "      <td>15.452262</td>\n",
       "      <td>0.072936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>15.139548</td>\n",
       "      <td>0.138511</td>\n",
       "      <td>15.138040</td>\n",
       "      <td>0.070778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>14.839479</td>\n",
       "      <td>0.137209</td>\n",
       "      <td>14.837476</td>\n",
       "      <td>0.067895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>14.554043</td>\n",
       "      <td>0.137167</td>\n",
       "      <td>14.550388</td>\n",
       "      <td>0.063020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>6.996687</td>\n",
       "      <td>0.045852</td>\n",
       "      <td>6.574947</td>\n",
       "      <td>0.035201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>6.996557</td>\n",
       "      <td>0.045878</td>\n",
       "      <td>6.574299</td>\n",
       "      <td>0.035081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>6.996372</td>\n",
       "      <td>0.045731</td>\n",
       "      <td>6.573820</td>\n",
       "      <td>0.035280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>6.996265</td>\n",
       "      <td>0.045465</td>\n",
       "      <td>6.573436</td>\n",
       "      <td>0.035340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>6.996331</td>\n",
       "      <td>0.045395</td>\n",
       "      <td>6.572938</td>\n",
       "      <td>0.035487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     iterations  test-RMSE-mean  test-RMSE-std  train-RMSE-mean  \\\n",
       "0             0       15.769091       0.144416        15.768537   \n",
       "1             1       15.453130       0.140421        15.452262   \n",
       "2             2       15.139548       0.138511        15.138040   \n",
       "3             3       14.839479       0.137209        14.837476   \n",
       "4             4       14.554043       0.137167        14.550388   \n",
       "..          ...             ...            ...              ...   \n",
       "995         995        6.996687       0.045852         6.574947   \n",
       "996         996        6.996557       0.045878         6.574299   \n",
       "997         997        6.996372       0.045731         6.573820   \n",
       "998         998        6.996265       0.045465         6.573436   \n",
       "999         999        6.996331       0.045395         6.572938   \n",
       "\n",
       "     train-RMSE-std  \n",
       "0          0.075459  \n",
       "1          0.072936  \n",
       "2          0.070778  \n",
       "3          0.067895  \n",
       "4          0.063020  \n",
       "..              ...  \n",
       "995        0.035201  \n",
       "996        0.035081  \n",
       "997        0.035280  \n",
       "998        0.035340  \n",
       "999        0.035487  \n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import cv, Pool\n",
    "params = {\n",
    "    \"loss_function\":\"RMSE\"\n",
    "}\n",
    "cv_dataset = Pool(data=X_train, label=y_train)\n",
    "cv(cv_dataset, params=params, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Light GBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/X/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3086\n",
      "[LightGBM] [Info] Number of data points in the train set: 130680, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 10.056688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48.45426553993418"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "d_train = lgb.Dataset(X_train, label = y_train)\n",
    "params = {}\n",
    "params['learning_rate'] = 0.33\n",
    "params['num_iterations'] = 100\n",
    "params['boosting_type'] = 'dart'\n",
    "params['objective'] = 'regression'\n",
    "params['metric'] = 'rmse'\n",
    "params['sub_feature'] = 0.6\n",
    "params['num_leaves'] = 50\n",
    "params['min_data'] = 60\n",
    "params['max_depth'] = 35\n",
    "clf = lgb.train(params, d_train, 100)\n",
    "y_pred=clf.predict(X_test)\n",
    "mean_squared_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test linear regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "reg = linear_model.TweedieRegressor().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test,reg_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test svm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model_svm =SVC(kernel='sigmoid', C=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pred = model_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test,svm_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test CNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submit import submit\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# from catboost import CatBoostRegressor\n",
    "\n",
    "model_cat = CatBoostRegressor(verbose=False)\n",
    "model_forest = RandomForestRegressor()\n",
    "model_xg = XGBRegressor(learning_rate=0.01, max_depth=5, objective=\"reg:linear\", subsample=0.7, n_estimators=500)\n",
    "\n",
    "submit(model_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/57/vt366_td3_37y7bqh4rk7nqr0000gn/T/ipykernel_11641/3681237703.py:3: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  X_train, y_train, X_test, y_test = get_submission_data()\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/X/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005061 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3113\n",
      "[LightGBM] [Info] Number of data points in the train set: 174241, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 10.087609\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_submission_data()\n",
    "X_train, X_test = normalize(X_train, X_test)\n",
    "\n",
    "\n",
    "d_train = lgb.Dataset(X_train, label = y_train)\n",
    "params = {}\n",
    "params['learning_rate'] = 0.10\n",
    "params['num_iterations'] = 500\n",
    "params['boosting_type'] = 'dart'\n",
    "params['objective'] = 'regression'\n",
    "params['metric'] = 'rmse'\n",
    "params['sub_feature'] = 0.6\n",
    "params['num_leaves'] = 50\n",
    "params['min_data'] = 60\n",
    "params['max_depth'] = 35\n",
    "clf = lgb.train(params, d_train, 100)\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "\n",
    "test, _ = get_test_data()\n",
    "test[\"hindex\"] = y_pred\n",
    "submission = test[[\"author\", \"hindex\"]]\n",
    "submission.to_csv(\"../tmp/submission.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f80d8b31525b6ea3adcf22249f5b1a58cdfe6d9f8b553721dcb1dc5ebfc8b9f9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
